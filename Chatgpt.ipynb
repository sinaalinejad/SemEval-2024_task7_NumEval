{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16Bbg2QS_fA3",
        "outputId": "f685a839-1065-4d97-92ee-0ed78704d05c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.4/225.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyVwqO5YLc-3",
        "outputId": "aca59891-f336-4fee-a303-2927aec907b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (8.2.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install tenacity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XoAWfUOH8qt",
        "outputId": "2231264c-d94a-4406-af5a-78cd1186ceef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9EEWuIzIHDx",
        "outputId": "26ad2e3b-1800-43cb-eb39-a46ccdd9209f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/SemEval\n"
          ]
        }
      ],
      "source": [
        "%cd \"./drive/MyDrive/SemEval/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIVewDxGIXRj",
        "outputId": "bd3b883a-c8d0-4e43-b667-051b2e61d9df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatgpt.ipynb  Cleaning.ipynb  Dev_Numerical_Reasoning.json  Train_Numerical_Reasoning.json\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "X7h5vOE__fA6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import config\n",
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "from tenacity import retry, stop_after_attempt, wait_fixed\n",
        "%run \"Cleaning.ipynb\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "-WlttjfS_fA7"
      },
      "outputs": [],
      "source": [
        "# Define the function to fill the blank in the headline\n",
        "@retry(stop=stop_after_attempt(3), wait=wait_fixed(20))\n",
        "def fill_blank(news, masked_headline, prompt, client):\n",
        "    # Construct the prompt by combining the news text and masked headline\n",
        "    prompt = prompt.replace(\"<NEWS>\", news).replace(\"<MASKED HEADLINE\", masked_headline)\n",
        "    messages = []\n",
        "    messages.append(\n",
        "        {\"role\": \"system\", \"content\": \"You are a reasonable and accurate assistant\"}\n",
        "    )\n",
        "    messages.append(\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    )\n",
        "    time.sleep(20)\n",
        "    chat = client.chat.completions.create(\n",
        "        messages=messages,\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "    )\n",
        "    reply = chat.choices[0].message.content\n",
        "    # Extract the generated text from the response and return it\n",
        "    return reply"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Bb0MJbo7_fA7"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "with open(\"./data/Train_Numerical_Reasoning.json\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Convert the list of dictionaries to a pandas DataFrame\n",
        "df = pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "iLRHm_r3_fA8"
      },
      "outputs": [],
      "source": [
        "mask_token = \"<MASK>\"\n",
        "df[\"news\"] = df[\"news\"].apply(replace_newline_tab)\n",
        "df[\"news\"] = df[\"news\"].apply(remove_commas)\n",
        "df[\"masked headline\"] = df[\"masked headline\"].apply(mask_text, args=(mask_token,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSmOcHCp_fA8",
        "outputId": "e8d41c95-e0a6-4c20-d24b-7c593150d3ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    <MASK>K Walmart Part-Timers to Lose Health Ins...\n",
              "1    Dax Shepard: Wedding to Kristen Bell Cost $<MASK>\n",
              "2                          Nancy Reagan Dead at <MASK>\n",
              "3    American Airlines Faces $<MASK>M Fine for Safe...\n",
              "4    $<MASK>K Raised for Kids of Mom Dismembered on...\n",
              "Name: masked headline, dtype: object"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"masked headline\"].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuWv9rH4_fA9",
        "outputId": "103c8817-356d-4f84-bb0b-9192bc2f0e62"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    (Oct 7, 2014  12:40 PM CDT) As of Jan. 1, Walm...\n",
              "1    (Oct 29, 2013  8:15 AM CDT) Dax Shepard and Kr...\n",
              "2    (Mar 6, 2016  10:50 AM) Nancy Reagan, the help...\n",
              "3    (Aug 15, 2008  5:11 AM CDT) American Airlines ...\n",
              "4    (Apr 18, 2016  1:02 PM CDT) Ingrid Lyne, the S...\n",
              "Name: news, dtype: object"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"news\"].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "JeVwMrDw_fA9"
      },
      "outputs": [],
      "source": [
        "def find_answer(text: str) -> str:\n",
        "  text = text.replace(\" \",\"\")\n",
        "  match = re.search(r\"<MASK>=([0-9]*[/|\\-|\\\\.]*[0-9]*)\", text)\n",
        "  if match:\n",
        "      number = match.group(1)\n",
        "      return number\n",
        "  match = re.search(r\"([0-9]+)\", text)\n",
        "  if match:\n",
        "    number = match.group(1)\n",
        "    return number\n",
        "  return -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "lbJbgC3P_fA9"
      },
      "outputs": [],
      "source": [
        "def test_on_chatgpt(sample, prompt, client, fill_blank, find_answer):\n",
        "  # Fill the blank in the headline for each item in the sample\n",
        "  sample[\"response\"] = sample.apply(\n",
        "      lambda row: fill_blank(row[\"news\"], row[\"masked headline\"], prompt, client), axis=1\n",
        "  )\n",
        "  sample[\"predicted_ans\"] = sample[\"response\"].apply(find_answer)\n",
        "  # Calculate the accuracy of ChatGPT's predictions\n",
        "  accuracy = (sample[\"ans\"] == sample[\"predicted_ans\"]).mean()\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "RHYbChZR_fA-"
      },
      "outputs": [],
      "source": [
        "# Set up OpenAI API credentials\n",
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key = config.API_KEY\n",
        ")\n",
        "# The prompt to feed into chatgpt\n",
        "prompt = \"I have a text of a news and its masked headline with mask token specified as <MASK>. The mask should be filled with a numerical value. you should just give me the numerical value to put instead of the mask. your response should be like this: <MASK> = <your predicted answer>\\nthe news is:\\n<NEWS>\\nthe masked headline is:\\n<MASKED HEADLINE>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bY-G810a_fA-"
      },
      "outputs": [],
      "source": [
        "# Sample 100 items from the dataset\n",
        "sample = df.sample(n=100, random_state=42)\n",
        "accuracy = test_on_chatgpt(sample, prompt, client, fill_blank, find_answer)\n",
        "print(f\"Accuracy: {accuracy:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "oPVFw-knX_Sm"
      },
      "outputs": [],
      "source": [
        "sample[['news', 'masked headline', 'ans', 'predicted_ans', 'calculation', 'response']].to_csv('chatgpt_output_2.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
